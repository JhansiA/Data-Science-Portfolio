{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.3\n"
     ]
    }
   ],
   "source": [
    "import rasa_nlu\n",
    "print(rasa_nlu.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu import config\n",
    "from rasa_nlu.config import RasaNLUModelConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "from  rasa_nlu.model import Metadata, Interpreter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas import*\n",
    "import csv\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from jieba import posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data for chinese messages\n",
    "df = pd.read_excel(\"excel.xlsx\", sheet_name= 'Sheet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function for message encoding\n",
    "def encode_decode(sentence):\n",
    "    sent1 = sentence.encode('Utf-8', 'replace')\n",
    "    sent2 = sent1.decode('Utf-8', 'replace')\n",
    "    return sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing english words and digits\n",
    "cleaned_data = []\n",
    "for i in df.Mention:\n",
    "    sent = re.sub(pattern , '', i)\n",
    "    cleaned_data.append(encode_decode(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mention_cleaned'] = cleaned_data # todo : Need to remove punctuation marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(): # load stopwords from txt file\n",
    "    file = open('stopwords-zh.txt','rb').read().decode('utf8').split('\\n')\n",
    "    return set(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(get_stop_words())\n",
    "punctuations = \"！？。＂#＃$＄％&＆'＇()（）*＊+＋，-－/／:：;；<＜=＝>＞@[［＼\\］] \\\n",
    "    ＾^＿_｀`{｛|｜}｝~～《》｟｠｢｣､、〃「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟 \\\n",
    "    〰〾〿–—‘’‛“”„‟…‧﹏.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for tokenizing giant string(stemming, removing stopwords and punctuation)\n",
    "def clean_sentence(sentence):\n",
    "    word_list = jieba.tokenize(sentence)\n",
    "    word_list = [word[0] for word in word_list if word[0] not in stopwords and word[0] not in punctuations]\n",
    "    new_sent = \" \".join(word_list)\n",
    "    return new_sent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether verb available in splitted sentence\n",
    "def has_verb(sentence, verbose=False):\n",
    "    pos_tagged = pseg.cut(sentence)\n",
    "    if verbose:\n",
    "        print(pos_tagged)\n",
    "    if 'v' in [tag for word,tag in pos_tagged]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for splitting sentences\n",
    "def custom_sentence_tokenizer(text, verbose=False):\n",
    "    # container for final result\n",
    "    new_split = []\n",
    "    \n",
    "    # Split sentences by a comma, 'and' and 'or'.\n",
    "    text_list = re.split('[。| ！| ？| 」]+', text) #[。| ！| ？| 」]+\\s\n",
    "    \n",
    "    # Remove white spaces and empty string elements from the list\n",
    "    text_list = [x.strip() for x in text_list]\n",
    "    #text_list = list(filter(None, text_list))\n",
    "        \n",
    "    # Append first list element to the new list.\n",
    "    new_split.append(text_list[0])\n",
    "    \n",
    "    # Check if the splits are valid sentences. If not, glue the parts together again.\n",
    "    for index in range(1, len(text_list)):\n",
    "        \n",
    "        # Keep the split if both parts of the sentences contain a verb.\n",
    "        if has_verb(text_list[index-1], verbose) and has_verb(text_list[index], verbose):\n",
    "            new_split.append(text_list[index])\n",
    "        # Glue the parts together again, since POS requirements are not met.\n",
    "        else:\n",
    "            new_split[-1] += ' ' + text_list[index]\n",
    "    \n",
    "    if verbose:\n",
    "        print('[.] Input sentence:')\n",
    "        print('    ', text)\n",
    "        print('[.] Output sentence(s):')\n",
    "        print('    ', new_split)\n",
    "    return new_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for splitting sentences\n",
    "def split_sentences_in_dataframe(df, verbose=False):\n",
    "    new_df = pd.DataFrame()\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Mention_cleaned']\n",
    "        text_list = custom_sentence_tokenizer(text, verbose)\n",
    "        for text_part in text_list:\n",
    "            new_row = row.copy()\n",
    "            clean_text_part = clean_sentence(text_part)\n",
    "            if(clean_text_part!= ''):\n",
    "                new_row['Mention_cleaned'] = clean_text_part\n",
    "                new_df = new_df.append(new_row)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = split_sentences_in_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df1[['Mention','Mention_cleaned','Intent']] #pd.DataFrame(columns=['Mention','Mention_cleaned','Intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert excel training data into rasa_nlu format\n",
    "def convert_excel_rasa_nlu(training_data):\n",
    "    results = []\n",
    "    common_examples_list = []\n",
    "    common_examples = []\n",
    "    x = []\n",
    "    for index, row in training_data.iterrows():\n",
    "        x = (row['Intent'],row['Mention_cleaned'],[])\n",
    "        results.append(x)\n",
    "    for result in results:\n",
    "        keys = ['intent', 'text', 'entities']\n",
    "        common_examples.append(dict(zip(keys, result)))\n",
    "    rasa_nlu_data1 = {\"regex_features\" : [], \"entity_synonyms\": [],\"common_examples\": []}\n",
    "    rasa_nlu_data1[\"common_examples\"] = common_examples\n",
    "    dataformat = dict(rasa_nlu_data = rasa_nlu_data1)\n",
    "    return(dataformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('rasa_dataset_chinese.json', 'w') as fp:\n",
    "    json.dump(convert_excel_rasa_nlu(training_data),fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training rasa model\n",
    "def train_nlu(data, configs, model_dir):\n",
    "    #print('in')\n",
    "    training_data = load_data(data)\n",
    "    trainer = Trainer(config.load(configs))\n",
    "    trainer.train(training_data)\n",
    "    model_directory = trainer.persist(model_dir, fixed_model_name = 'rasaTLnlu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\rasa_nlu\\training_data\\training_data.py:186: UserWarning: Intent 'temperature' has only 1 training examples! Minimum is 2, training may fail.\n",
      "  self.MIN_EXAMPLES_PER_INTENT))\n",
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Restoring parameters from ./models/default/rasaTLnlu\\intent_classifier_tensorflow_embedding.ckpt\n"
     ]
    }
   ],
   "source": [
    "# will take few minutes\n",
    "if __name__ == '__main__':\n",
    "    train_nlu('rasa_dataset_chinese.json', 'config_rasa_chinese.yaml', './models/')\n",
    "    interpreter = Interpreter.load('./models/default/rasaTLnlu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data to predict intent of the message\n",
    "df_test = pd.read_excel(\"test_excel.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "for i in df_test.Mention:\n",
    "    sent = re.sub(pattern , '', i)\n",
    "    cleaned_data.append(encode_decode(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Mention_cleaned'] = cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1 = split_sentences_in_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_1.id = df_test_1.id.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the intent of message\n",
    "def predict_text_intent(intent_result, id_pred):\n",
    "    #global df_rasa_output\n",
    "    if intent_result['intent']['name'] in ['spam']:\n",
    "        #check if it is a spam comment, if yes then append into the final dataframe\n",
    "        df1 = pd.DataFrame(columns=['id','mention','confidence','intent'])\n",
    "        extract_text = intent_result['text']\n",
    "        extract_confidence = intent_result['intent']['confidence']\n",
    "        extract_intent = intent_result['intent']['name']\n",
    "        if extract_confidence > 0.5:\n",
    "            row = [id_pred, extract_text, extract_confidence, extract_intent]\n",
    "            df1.loc[len(df1)] = row\n",
    "            df1.append(row, ignore_index=True)\n",
    "        #df_rasa_output = df_rasa_output.append(pd.DataFrame(data = df1), ignore_index=True)\n",
    "    else:\n",
    "        df1 = pd.DataFrame(columns=['id','mention','confidence','intent'])\n",
    "        count_intent_range = len(intent_result['intent_ranking'])\n",
    "        intent_ranking_result = intent_result['intent_ranking']\n",
    "        for i in range(count_intent_range):\n",
    "            string = []\n",
    "            index_range = i\n",
    "            string = intent_ranking_result[index_range]\n",
    "            extract_text = intent_result['text']\n",
    "            extract_confidence = string['confidence']\n",
    "            extract_intent = string['name']\n",
    "            if extract_confidence > 0.5:\n",
    "                row = [id_pred, extract_text, extract_confidence, extract_intent]\n",
    "                df1.loc[len(df1)] = row\n",
    "                df1.append(row, ignore_index=True)\n",
    "            #df_rasa_output = df_rasa_output.append(pd.DataFrame(data = df1), ignore_index=True)\n",
    "    return df1#df_rasa_output.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\api.py:107: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    }
   ],
   "source": [
    "test_dataset_count = df_test_1.Mention_cleaned.count()\n",
    "df_rasa_output = pd.DataFrame(columns=['id','mention','confidence','intent'])\n",
    "for i in range(test_dataset_count):\n",
    "    extract_chatlog = df_test_1.Mention_cleaned.iloc[i]\n",
    "    if extract_chatlog != '':\n",
    "        classify_chat = interpreter.parse(extract_chatlog)\n",
    "        id_pred = df_test_1.id.iloc[i]\n",
    "        df1 = predict_text_intent(classify_chat, id_pred)    \n",
    "        df_rasa_output = df_rasa_output.append(pd.DataFrame(data = df1), ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
