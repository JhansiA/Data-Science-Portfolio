{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Importing the required modules for NLP\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import re\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess #text processing\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('wordnet')\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data from source excel\n",
    "train_df = pd.read_excel('Thinkcol_case_study.xlsx',sheet_name='train')\n",
    "test_df = pd.read_excel('Thinkcol_case_study.xlsx',sheet_name='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing basic exploration on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1743 entries, 0 to 1742\n",
      "Data columns (total 4 columns):\n",
      "ID          1743 non-null int64\n",
      "Mention     1743 non-null object\n",
      "Target      1743 non-null int64\n",
      "Category    1743 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 54.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48415 entries, 0 to 48414\n",
      "Data columns (total 4 columns):\n",
      "id            48415 non-null int64\n",
      "Message ID    48415 non-null int64\n",
      "Date          48415 non-null datetime64[ns]\n",
      "Mention       48385 non-null object\n",
      "dtypes: datetime64[ns](1), int64(2), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dropna(axis = 0, subset=['Mention'],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1666\n",
       "1      77\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Target'].value_counts() #imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\",\"😍\",\"❤️\",\"😐\",\"✌️\",\"❤\",\"👊\",\"✋\",\"📱📲\",\"👌\",\"🚂\",\"📱➡️\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the reviews in a list and preparing predictors and target\n",
    "X=train_df.loc[:,\"Mention\"].copy()\n",
    "y=train_df.loc[:,\"Target\"].copy()\n",
    "test=train_df.loc[:,\"Mention\"].copy()\n",
    "# Splitting the train data into train and validation data\n",
    "from  sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.33, random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a customized stopwords file for portugese language to remove unwanted words\n",
    "txt = pd.read_csv('stopwords_portuguese.txt', sep=\" \", header=None)\n",
    "newstoplist=txt[0].tolist()\n",
    "stopwords.extend(newstoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will handle cleaning the text, tokenization and lemmatization\n",
    "def Pre_processing(text):\n",
    "    corpus_raw=text.copy()\n",
    "    corpus=list(corpus_raw.values)\n",
    "    sentences =  list(filter(None, corpus))\n",
    "    #doing some simple preprocessing\n",
    "    process_list=[]\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            process_list.append(simple_preprocess(sentence))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    new_list=[]  \n",
    "    #Tokenization and removal of stopwords and punctuations\n",
    "    for sent in process_list:\n",
    "        newsent=\" \".join(sent)\n",
    "        word_tokens = word_tokenize(newsent)\n",
    "        filtered_sentence = [w for w in word_tokens if w not in stopwords and w not in punctuations]\n",
    "        #print(filtered_sentence)\n",
    "        new_list.append(filtered_sentence)\n",
    "        \n",
    "    #introducing lemmatization (to remove inflectional endings only and to return the base using vocabulary)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    new_list2=[]\n",
    "    for sent in new_list:\n",
    "        normalized = \" \".join(lemma.lemmatize(word,'n').lower() for word in sent)\n",
    "        x = normalized.split()\n",
    "        y = [s for s in x if len(s) > 2]\n",
    "        new_list2.append(y)   \n",
    "     #Preparing for vectorization\n",
    "    text =[]\n",
    "    for sent in new_list2:\n",
    "        a = \" \".join(word for word in sent )\n",
    "        text.append(a)\n",
    "    return  text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF : To give more weightage to the important rather than frequent words like The, is ...\n",
    "tfidf_vectorizer = TfidfVectorizer(input = Pre_processing ,stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe to store the evaluation metrics for each machine learning algorithm\n",
    "Eval_metrics = pd.DataFrame(columns= ['Model','Precision','Recall','F1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help function for evaluation metrics\n",
    "def classification_report_df(report, name):\n",
    "    dict = {}\n",
    "    lines = report.split('\\n')\n",
    "    row_data = lines[-2].split('      ')\n",
    "    dict = {'Model':name,'Precision' : float(row_data[1]),'Recall' : float(row_data[2]),'F1-score' : float(row_data[3]) }\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules and classification models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "models.append(('XGB', XGBClassifier()))\n",
    "models.append(('LGB', LGBMClassifier()))\n",
    "models.append(('NB', MultinomialNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.956597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 0.986111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB: 0.984375\n",
      "NB: 0.956597\n",
      "SVM: 0.956597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Training the data for different machine learning classification algorithms\n",
    "for name , clf in models:\n",
    "    pipe = Pipeline([('data', tfidf_vectorizer), ('clf', clf)])\n",
    "    # training\n",
    "    pipe.fit(X_train, y_train)\n",
    "    # validation test\n",
    "    preds = pipe.predict(X_test)\n",
    "    #Evaluating the model\n",
    "    report = metrics.classification_report(y_test, preds)\n",
    "    Evaluations = classification_report_df(report,name)\n",
    "    Eval_metrics = Eval_metrics.append(Evaluations,ignore_index= True)        \n",
    "    msg = \"%s: %f\" % (name, accuracy_score(y_test, preds))\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGB</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision  Recall  F1-score\n",
       "0  Logistic Regression       0.92    0.96      0.94\n",
       "1                  XGB       0.99    0.99      0.98\n",
       "2                  LGB       0.98    0.98      0.98\n",
       "3                   NB       0.92    0.96      0.94\n",
       "4                  SVM       0.92    0.96      0.94"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the metrics for each classification model\n",
    "Eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       551\n",
      "          1       1.00      0.68      0.81        25\n",
      "\n",
      "avg / total       0.99      0.99      0.98       576\n",
      "\n",
      "acurracy: 0.986111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#From the above evaluation metrics using XGBoost as best model and training the data only using XGboost\n",
    "clf = XGBClassifier()\n",
    "pipe = Pipeline([('data', tfidf_vectorizer), ('clf', clf)])\n",
    "# training the data\n",
    "pipe.fit(X_train, y_train)\n",
    "# validating the data\n",
    "preds = pipe.predict(X_test)\n",
    "print(metrics.classification_report(y_test, preds))\n",
    "msg = \"%s: %f\" % ('accuracy', accuracy_score(y_test, preds))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhansi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Predicting the target variable for main test data\n",
    "predictions = pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just looking the test data predictions to cross verify manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,    5,   19, ..., 1740, 1741, 1742], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(predictions == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nSacanagem isso estão parecendo a Apple lançam aparelhos e param de dar suporte logo em seguida',\n",
       " '  de  un  reproductor  de  pelcula  donde  se  Nesta moldura ou suporte de placa existem dois flashes de xennio embutidos e um sensor de flash   um Elemento em uma Moto mandou eu parar  como no Parei  Isso frequentemente acontece quando deixamos o nosso celular no silencioso ou em um toque   voc ser convidado a ensinar como o Call Flash',\n",
       " 'estou com o mesmo problema no meu moto g5 platinum você conseguiu resolver isso na assistencia quais foram os procedimentos ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1740:1743] # pred == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moto G5 plus azul safira e com apenas 1 semana de uso o mesmo apresentou defeito o CHIP 2 não tem sinal e pela pesquisa que fiz este defeito é característico do moto g então entrei em contato com a Motorola para resolução do meu problema através do chat a mesma passou alguns procedimentos que não deram certo e então solicitaram que eu enviasse o celular para a assistência técnica que fica 400km da',\n",
       " ' Levei em uma autorizada fora da minha cidade e não conseguiram encontrar o defeito',\n",
       " 'Comprei um lenovo k6 plus em maio desse ano e o mesmo já apresentou defeito os botões home multitarefa e voltar pararam de funcionar e para piorar não tem uma assistência técnica da motorola em Joinville SC é muito chato comprar um celular que com 5 meses apresenta defeito totalmente decepcionado com a marca']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:3] # pred == 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
